{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642c1c05-a0b2-4f7c-b6cd-06f6130f4e04",
   "metadata": {},
   "source": [
    "- modify load_image to eventually add center crop\n",
    "- add warning of fixed size for ViT and CLIP\n",
    "- Use CLIP CNN without cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af67bae-6931-4cf2-a0f9-2e328f5848e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I always like to structure my imports into Python's native libs,\n",
    "# stuff I installed via conda/pip and local file imports (but we don't have those here)\n",
    "\n",
    "# Python native libs\n",
    "import os\n",
    "import enum\n",
    "from collections import namedtuple\n",
    "import argparse\n",
    "import numbers\n",
    "import math\n",
    "\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "import open_clip\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # visualizations\n",
    "from PIL import Image\n",
    "\n",
    "import utils.constants as constants\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bbf63f-6990-4d24-bf60-84c7a0a20e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(constants)\n",
    "\n",
    "from utils.constants import *\n",
    "from utils.utils import *\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2ed1e-ab4e-4a94-b962-5f2085265193",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## smongolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28d97d9-f885-4c79-8bf0-4cae4a5ad40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"OPENCLIPCOCA_BASE\" : \"coca_base\",\n",
      "    \"OPENCLIPCOCA_ROBERTA_VIT_B_32\" : \"coca_roberta-ViT-B-32\",\n",
      "    \"OPENCLIPCOCA_VIT_B_32\" : \"coca_ViT-B-32\",\n",
      "    \"OPENCLIPCOCA_VIT_L_14\" : \"coca_ViT-L-14\",\n",
      "    \"OPENCLIPCONVNEXT_BASE\" : \"convnext_base\",\n",
      "    \"OPENCLIPCONVNEXT_BASE_W\" : \"convnext_base_w\",\n",
      "    \"OPENCLIPCONVNEXT_BASE_W_320\" : \"convnext_base_w_320\",\n",
      "    \"OPENCLIPCONVNEXT_LARGE\" : \"convnext_large\",\n",
      "    \"OPENCLIPCONVNEXT_LARGE_D\" : \"convnext_large_d\",\n",
      "    \"OPENCLIPCONVNEXT_LARGE_D_320\" : \"convnext_large_d_320\",\n",
      "    \"OPENCLIPCONVNEXT_SMALL\" : \"convnext_small\",\n",
      "    \"OPENCLIPCONVNEXT_TINY\" : \"convnext_tiny\",\n",
      "    \"OPENCLIPCONVNEXT_XLARGE\" : \"convnext_xlarge\",\n",
      "    \"OPENCLIPCONVNEXT_XXLARGE\" : \"convnext_xxlarge\",\n",
      "    \"OPENCLIPCONVNEXT_XXLARGE_320\" : \"convnext_xxlarge_320\",\n",
      "    \"OPENCLIPEVA01_G_14\" : \"EVA01-g-14\",\n",
      "    \"OPENCLIPEVA01_G_14_PLUS\" : \"EVA01-g-14-plus\",\n",
      "    \"OPENCLIPEVA02_B_16\" : \"EVA02-B-16\",\n",
      "    \"OPENCLIPEVA02_E_14\" : \"EVA02-E-14\",\n",
      "    \"OPENCLIPEVA02_E_14_PLUS\" : \"EVA02-E-14-plus\",\n",
      "    \"OPENCLIPEVA02_L_14\" : \"EVA02-L-14\",\n",
      "    \"OPENCLIPEVA02_L_14_336\" : \"EVA02-L-14-336\",\n",
      "    \"OPENCLIPMT5_BASE_VIT_B_32\" : \"mt5-base-ViT-B-32\",\n",
      "    \"OPENCLIPMT5_XL_VIT_H_14\" : \"mt5-xl-ViT-H-14\",\n",
      "    \"OPENCLIPRN50\" : \"RN50\",\n",
      "    \"OPENCLIPRN50_QUICKGELU\" : \"RN50-quickgelu\",\n",
      "    \"OPENCLIPRN50X4\" : \"RN50x4\",\n",
      "    \"OPENCLIPRN50X16\" : \"RN50x16\",\n",
      "    \"OPENCLIPRN50X64\" : \"RN50x64\",\n",
      "    \"OPENCLIPRN101\" : \"RN101\",\n",
      "    \"OPENCLIPRN101_QUICKGELU\" : \"RN101-quickgelu\",\n",
      "    \"OPENCLIPROBERTA_VIT_B_32\" : \"roberta-ViT-B-32\",\n",
      "    \"OPENCLIPSWIN_BASE_PATCH4_WINDOW7_224\" : \"swin_base_patch4_window7_224\",\n",
      "    \"OPENCLIPVIT_B_16\" : \"ViT-B-16\",\n",
      "    \"OPENCLIPVIT_B_16_PLUS\" : \"ViT-B-16-plus\",\n",
      "    \"OPENCLIPVIT_B_16_PLUS_240\" : \"ViT-B-16-plus-240\",\n",
      "    \"OPENCLIPVIT_B_32\" : \"ViT-B-32\",\n",
      "    \"OPENCLIPVIT_B_32_PLUS_256\" : \"ViT-B-32-plus-256\",\n",
      "    \"OPENCLIPVIT_B_32_QUICKGELU\" : \"ViT-B-32-quickgelu\",\n",
      "    \"OPENCLIPVIT_BIGG_14\" : \"ViT-bigG-14\",\n",
      "    \"OPENCLIPVIT_E_14\" : \"ViT-e-14\",\n",
      "    \"OPENCLIPVIT_G_14\" : \"ViT-g-14\",\n",
      "    \"OPENCLIPVIT_H_14\" : \"ViT-H-14\",\n",
      "    \"OPENCLIPVIT_H_16\" : \"ViT-H-16\",\n",
      "    \"OPENCLIPVIT_L_14\" : \"ViT-L-14\",\n",
      "    \"OPENCLIPVIT_L_14_280\" : \"ViT-L-14-280\",\n",
      "    \"OPENCLIPVIT_L_14_336\" : \"ViT-L-14-336\",\n",
      "    \"OPENCLIPVIT_L_16\" : \"ViT-L-16\",\n",
      "    \"OPENCLIPVIT_L_16_320\" : \"ViT-L-16-320\",\n",
      "    \"OPENCLIPVIT_M_16\" : \"ViT-M-16\",\n",
      "    \"OPENCLIPVIT_M_16_ALT\" : \"ViT-M-16-alt\",\n",
      "    \"OPENCLIPVIT_M_32\" : \"ViT-M-32\",\n",
      "    \"OPENCLIPVIT_M_32_ALT\" : \"ViT-M-32-alt\",\n",
      "    \"OPENCLIPVIT_S_16\" : \"ViT-S-16\",\n",
      "    \"OPENCLIPVIT_S_16_ALT\" : \"ViT-S-16-alt\",\n",
      "    \"OPENCLIPVIT_S_32\" : \"ViT-S-32\",\n",
      "    \"OPENCLIPVIT_S_32_ALT\" : \"ViT-S-32-alt\",\n",
      "    \"OPENCLIPVIT_MEDIUM_PATCH16_GAP_256\" : \"vit_medium_patch16_gap_256\",\n",
      "    \"OPENCLIPVIT_RELPOS_MEDIUM_PATCH16_CLS_224\" : \"vit_relpos_medium_patch16_cls_224\",\n",
      "    \"OPENCLIPXLM_ROBERTA_BASE_VIT_B_32\" : \"xlm-roberta-base-ViT-B-32\",\n",
      "    \"OPENCLIPXLM_ROBERTA_LARGE_VIT_H_14\" : \"xlm-roberta-large-ViT-H-14\",\n"
     ]
    }
   ],
   "source": [
    "model_names = open_clip.list_models()\n",
    "for model_name in model_names:\n",
    "    u_name = \"OPENCLIP\"+model_name.upper().replace(\"-\", \"_\")\n",
    "    print(f'    \"{u_name}\" : \"{model_name}\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a3e66ad-8776-48b3-8af3-866f3e252328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openai', 'yfcc15m', 'cc12m']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained_tags_by_model(\"RN50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d2484ab-789f-4168-9c4a-93b6ddd55d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[24.6250, 23.1875]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"RN50\", device=device)\n",
    "\n",
    "input_img_name = 'figures.jpg'  # checked in, I'll be using it as the running example\n",
    "img_path = os.path.join(INPUT_DATA_PATH, input_img_name)\n",
    "\n",
    "image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"two robots\", \"three robots\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #image_features = model.encode_image(image)\n",
    "    #text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", logits_per_image)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2893d8f4-8f1f-46e2-ad57-7de1a76bb9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CC12M = 2\n",
      "    COMMONPOOL_L_BASIC_S1B_B8K = 3\n",
      "    COMMONPOOL_L_CLIP_S1B_B8K = 4\n",
      "    COMMONPOOL_L_IMAGE_S1B_B8K = 5\n",
      "    COMMONPOOL_L_LAION_S1B_B8K = 6\n",
      "    COMMONPOOL_L_S1B_B8K = 7\n",
      "    COMMONPOOL_L_TEXT_S1B_B8K = 8\n",
      "    COMMONPOOL_M_BASIC_S128M_B4K = 9\n",
      "    COMMONPOOL_M_CLIP_S128M_B4K = 10\n",
      "    COMMONPOOL_M_IMAGE_S128M_B4K = 11\n",
      "    COMMONPOOL_M_LAION_S128M_B4K = 12\n",
      "    COMMONPOOL_M_S128M_B4K = 13\n",
      "    COMMONPOOL_M_TEXT_S128M_B4K = 14\n",
      "    COMMONPOOL_S_BASIC_S13M_B4K = 15\n",
      "    COMMONPOOL_S_CLIP_S13M_B4K = 16\n",
      "    COMMONPOOL_S_IMAGE_S13M_B4K = 17\n",
      "    COMMONPOOL_S_LAION_S13M_B4K = 18\n",
      "    COMMONPOOL_S_S13M_B4K = 19\n",
      "    COMMONPOOL_S_TEXT_S13M_B4K = 20\n",
      "    COMMONPOOL_XL_CLIP_S13B_B90K = 21\n",
      "    COMMONPOOL_XL_LAION_S13B_B90K = 22\n",
      "    COMMONPOOL_XL_S13B_B90K = 23\n",
      "    DATACOMP_L_S1B_B8K = 24\n",
      "    DATACOMP_M_S128M_B4K = 25\n",
      "    DATACOMP_S_S13M_B4K = 26\n",
      "    DATACOMP_XL_S13B_B90K = 27\n",
      "    FROZEN_LAION5B_S13B_B90K = 28\n",
      "    LAION2B_E16 = 29\n",
      "    LAION2B_S12B_B32K = 30\n",
      "    LAION2B_S12B_B42K = 31\n",
      "    LAION2B_S13B_B82K = 32\n",
      "    LAION2B_S13B_B82K_AUGREG = 33\n",
      "    LAION2B_S13B_B90K = 34\n",
      "    LAION2B_S26B_B102K_AUGREG = 35\n",
      "    LAION2B_S29B_B131K_FT = 36\n",
      "    LAION2B_S29B_B131K_FT_SOUP = 37\n",
      "    LAION2B_S32B_B79K = 38\n",
      "    LAION2B_S32B_B82K = 39\n",
      "    LAION2B_S34B_B79K = 40\n",
      "    LAION2B_S34B_B82K_AUGREG = 41\n",
      "    LAION2B_S34B_B82K_AUGREG_REWIND = 42\n",
      "    LAION2B_S34B_B82K_AUGREG_SOUP = 43\n",
      "    LAION2B_S34B_B88K = 44\n",
      "    LAION2B_S39B_B160K = 45\n",
      "    LAION2B_S4B_B115K = 46\n",
      "    LAION2B_S9B_B144K = 47\n",
      "    LAION400M_E31 = 48\n",
      "    LAION400M_E32 = 49\n",
      "    LAION400M_S11B_B41K = 50\n",
      "    LAION400M_S13B_B51K = 51\n",
      "    LAION5B_S13B_B90K = 52\n",
      "    LAION_AESTHETIC_S13B_B82K = 53\n",
      "    LAION_AESTHETIC_S13B_B82K_AUGREG = 54\n",
      "    MERGED2B_S11B_B114K = 55\n",
      "    MERGED2B_S4B_B131K = 56\n",
      "    MERGED2B_S6B_B61K = 57\n",
      "    MERGED2B_S8B_B131K = 58\n",
      "    MSCOCO_FINETUNED_LAION2B_S13B_B90K = 59\n",
      "    OPENAI = 60\n",
      "    YFCC15M = 61\n"
     ]
    }
   ],
   "source": [
    "pretrained = open_clip.list_pretrained()\n",
    "weights = list(np.unique([x[1] for x in pretrained]))\n",
    "up_weights = [x.upper() for x in weights]\n",
    "for i, x in enumerate(up_weights):\n",
    "    print(\"    \"+x+\" = \"+str(i+2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb97564-4ec7-40d2-8530-5a246f42d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RN50\"\n",
    "\n",
    "model, preprocess = clip.load(model_name, device=device)\n",
    "input_img_name = 'figures.jpg'  # checked in, I'll be using it as the running example\n",
    "img_path = os.path.join(INPUT_DATA_PATH, input_img_name)\n",
    "shape = FIxedImageResolutions[model_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e1155247-88fd-44ce-9915-2c892d58fd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bc0fddfe584f5a8a66145f2d255c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)ip_pytorch_model.bin:   0%|          | 0.00/718M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('convnext_base_w_320', pretrained='laion_aesthetic_s13b_b82k_augreg')\n",
    "tokenizer = open_clip.get_tokenizer('convnext_base_w_320')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77cd33ba-9d96-4d28-a731-d8b1e5c294c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(img_path)).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7c144e2-345e-424c-a80d-9cd211e322ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_to_rgb at 0x7f38e8f8d430>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de541e-dbfa-4807-a6b5-cae1f398064a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Some utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4710c4-2f54-42c7-bcf1-39c926773a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import *\n",
    "from utils.utils import *\n",
    "from deepdream import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\n",
    "\n",
    "class SupportedModels(enum.Enum):\n",
    "    VGG16 = 0\n",
    "    VGG16_EXPERIMENTAL = 1\n",
    "    GOOGLENET = 2\n",
    "    RESNET50 = 3\n",
    "    ALEXNET = 4\n",
    "    VIT = 5\n",
    "    CLIPVITB32 = 6\n",
    "    CLIPVITB16 = 7\n",
    "    CLIPVITL14 = 8\n",
    "    CLIPVITL14_336 = 9\n",
    "    CLIPRN50 = 10\n",
    "    CLIPRN101 = 11\n",
    "    CLIPRN50x4 = 12\n",
    "    CLIPRN50x16 = 13\n",
    "    CLIPRN50x64 = 14\n",
    "    \n",
    "\n",
    "def fetch_and_prepare_model(model_type, pretrained_weights, device):\n",
    "    if model_type == SupportedModels.VGG16.name:\n",
    "        model = Vgg16(pretrained_weights, requires_grad=False, show_progress=True).to(device)\n",
    "    elif model_type == SupportedModels.VGG16_EXPERIMENTAL.name:\n",
    "        model = Vgg16Experimental(pretrained_weights, requires_grad=False, show_progress=True).to(device)\n",
    "    elif model_type == SupportedModels.GOOGLENET.name:\n",
    "        model = GoogLeNet(pretrained_weights, requires_grad=False, show_progress=True).to(device)\n",
    "    elif model_type == SupportedModels.RESNET50.name:\n",
    "        model = ResNet50(pretrained_weights, requires_grad=False, show_progress=True).to(device)\n",
    "    elif model_type == SupportedModels.ALEXNET.name:\n",
    "        model = AlexNet(pretrained_weights, requires_grad=False, show_progress=True).to(device)\n",
    "    elif model_type == SupportedModels.VIT.name:\n",
    "        model = ViT(pretrained_weights, requires_grad=False, show_progress=True).to(device)\n",
    "    elif model_type == SupportedModels.CLIPVITB32.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"ViT-B/32\").to(device)\n",
    "    elif model_type == SupportedModels.CLIPVITB16.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"ViT-B/16\").to(device)\n",
    "    elif model_type == SupportedModels.CLIPVITL14.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"ViT-L/14\").to(device)\n",
    "    elif model_type == SupportedModels.CLIPVITL14_336.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"ViT-L/14@336px\").to(device)\n",
    "    elif model_type == SupportedModels.CLIPRN50.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"RN50\").to(device)\n",
    "    elif model_type == SupportedModels.CLIPRN101.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"RN101\").to(device)\n",
    "    elif model_type == SupportedModels.CLIPRN50x4.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"RN50x4\").to(device)\n",
    "    elif model_type == SupportedModels.CLIPRN50x16.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"RN50x16\").to(device)\n",
    "    elif model_type == SupportedModels.CLIPRN50x64.name:\n",
    "        model = CLIP(requires_grad = False, show_progress = True, checkpoint = \"RN50x64\").to(device)\n",
    "    else:\n",
    "        raise Exception('Model not yet supported.')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c537ef0-1936-4f10-9f96-5f9ff08601ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19168be8-e964-4248-bc4c-9c5f99fbabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward(layer) <- original implementation did it like this it's equivalent to MSE(reduction='sum')/2\n",
    "def gradient_ascent(config, model, input_tensor, input_text, layer_ids_to_use, iteration):\n",
    "    # Step 0: Feed forward pass\n",
    "    if model.__class__.__name__ == \"CLIP\":\n",
    "        out = model((input_tensor, input_text))\n",
    "    else:\n",
    "        out = model(input_tensor)\n",
    "\n",
    "    # Step 1: Grab activations/feature maps of interest\n",
    "    activations = [out[layer_id_to_use] for layer_id_to_use in layer_ids_to_use]\n",
    "\n",
    "    # Step 2: Calculate loss over activations\n",
    "    losses = []\n",
    "    for layer_activation in activations:\n",
    "        # Use torch.norm(torch.flatten(layer_activation), p) with p=2 for L2 loss and p=1 for L1 loss.\n",
    "        # But I'll use the MSE as it works really good, I didn't notice any serious change when going to L1/L2.\n",
    "        # using torch.zeros_like as if we wanted to make activations as small as possible but we'll do gradient ascent\n",
    "        # and that will cause it to actually amplify whatever the network \"sees\" thus yielding the famous DeepDream look\n",
    "        loss_component = torch.nn.MSELoss(reduction='mean')(layer_activation, torch.zeros_like(layer_activation))\n",
    "        losses.append(loss_component)\n",
    "\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 3: Process image gradients (smoothing + normalization)\n",
    "    grad = input_tensor.grad.data\n",
    "\n",
    "    # Applies 3 Gaussian kernels and thus \"blurs\" or smoothens the gradients and gives visually more pleasing results\n",
    "    # sigma is calculated using an arbitrary heuristic feel free to experiment\n",
    "    sigma = ((iteration + 1) / config['num_gradient_ascent_iterations']) * 2.0 + config['smoothing_coefficient']\n",
    "    smooth_grad = utils.CascadeGaussianSmoothing(kernel_size=9, sigma=sigma)(grad)  # \"magic number\" 9 just works well\n",
    "\n",
    "    # Normalize the gradients (make them have mean = 0 and std = 1)\n",
    "    # I didn't notice any big difference normalizing the mean as well - feel free to experiment\n",
    "    g_std = torch.std(smooth_grad)\n",
    "    g_mean = torch.mean(smooth_grad)\n",
    "    smooth_grad = smooth_grad - g_mean\n",
    "    smooth_grad = smooth_grad / g_std\n",
    "\n",
    "    # Step 4: Update image using the calculated gradients (gradient ascent step)\n",
    "    input_tensor.data += config['lr'] * smooth_grad\n",
    "\n",
    "    # Step 5: Clear gradients and clamp the data (otherwise values would explode to +- \"infinity\")\n",
    "    input_tensor.grad.data.zero_()\n",
    "    input_tensor.data = torch.max(torch.min(input_tensor, ConstantsContext.UPPER_IMAGE_BOUND), ConstantsContext.LOWER_IMAGE_BOUND)\n",
    "\n",
    "\n",
    "def deep_dream_static_image(config, img=None):\n",
    "    model = fetch_and_prepare_model(config['model_name'], config['pretrained_weights'], DEVICE)\n",
    "\n",
    "    try:\n",
    "        layer_ids_to_use = [model.layer_names.index(layer_name) for layer_name in config['layers_to_use']]\n",
    "    except Exception as e:  # making sure you set the correct layer name for this specific model\n",
    "        print(f'Invalid layer names {[layer_name for layer_name in config[\"layers_to_use\"]]}.')\n",
    "        print(f'Available layers for model {config[\"model_name\"]} are {model.layer_names}.')\n",
    "        return\n",
    "\n",
    "    if img is None:  # load either the provided image or start from a pure noise image\n",
    "        img_path = os.path.join(INPUT_DATA_PATH, config['input'])\n",
    "        # load a numpy, [0, 1] range, channel-last, RGB image\n",
    "        img = load_image(img_path, target_shape=config['img_width'])\n",
    "        if config['use_noise']:\n",
    "            shape = img.shape\n",
    "            img = np.random.uniform(low=0.0, high=1.0, size=shape).astype(np.float32)\n",
    "\n",
    "    img = pre_process_numpy_img(img)\n",
    "    original_shape = img.shape[:-1]  # save initial height and width\n",
    "\n",
    "    input_text = config[\"text_prompt\"]\n",
    "\n",
    "    print(\"====\")\n",
    "    print(model.__class__.__name__)\n",
    "\n",
    "    # Note: simply rescaling the whole result (and not only details, see original implementation) gave me better results\n",
    "    # Going from smaller to bigger resolution (from pyramid top to bottom)\n",
    "    for pyramid_level in range(config['pyramid_size']):\n",
    "        new_shape = get_new_shape(config, original_shape, pyramid_level)\n",
    "        img = cv.resize(img, (new_shape[1], new_shape[0]))  # resize depending on the current pyramid level\n",
    "        img = pad_image_to_shape(img, original_shape)\n",
    "        input_tensor = pytorch_input_adapter(img, DEVICE)  # convert to trainable tensor\n",
    "\n",
    "        print(\"======\")\n",
    "        print(f\"pyramid_level = {pyramid_level}\")\n",
    "        print(f\"new_shape = {new_shape, input_tensor.shape}\")\n",
    "        \n",
    "        for iteration in range(config['num_gradient_ascent_iterations']):\n",
    "            \n",
    "            # Introduce some randomness, it will give us more diverse results especially when you're making videos\n",
    "            h_shift, w_shift = np.random.randint(-config['spatial_shift_size'], config['spatial_shift_size'] + 1, 2)\n",
    "            input_tensor = random_circular_spatial_shift(input_tensor, h_shift, w_shift)\n",
    "\n",
    "            # This is where the magic happens, treat it as a black box until the next cell\n",
    "            gradient_ascent(config, model, input_tensor, input_text, layer_ids_to_use, iteration)\n",
    "\n",
    "            # Roll back by the same amount as above (hence should_undo=True)\n",
    "            input_tensor = random_circular_spatial_shift(input_tensor, h_shift, w_shift, should_undo=True)\n",
    "\n",
    "        img = pytorch_output_adapter(input_tensor)\n",
    "        img = extract_original_from_padded(img, new_shape)\n",
    "        plt.imshow(post_process_numpy_img(img))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    return post_process_numpy_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b25734-734a-4f60-8b34-0d928d8c664d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a5c05c7-8140-4031-9d4f-6de82c87f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(torch.nn.Module):\n",
    "    def __init__(self, pretrained_weights, requires_grad=False, show_progress=False):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained_weights == SupportedPretrainedWeights.IMAGENET.name:\n",
    "            vit = models.vit_b_16(pretrained=True, progress = True).eval().to(DEVICE)\n",
    "        else:\n",
    "            raise Exception(f'Pretrained weights {pretrained_weights} not yet supported for {self.__class__.__name__} model.')\n",
    "        \n",
    "        self.process_input = vit._process_input\n",
    "        self.class_token = vit.class_token\n",
    "        self.pos_embedding = vit.encoder.pos_embedding\n",
    "        \n",
    "        self.num_layers = 12\n",
    "        self.layers = vit.encoder.layers\n",
    "\n",
    "        self.layers_of_interest = [11]\n",
    "        self.layer_names = [\"self_attention\", \"gelu\"]\n",
    "\n",
    "        # Set these to False so that PyTorch won't be including them in it's autograd engine - eating up precious memory\n",
    "        if not requires_grad:\n",
    "          for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.process_input(x)\n",
    "        # N x 196 x 768\n",
    "        n = x.shape[0]\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n,-1,-1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "        # N x 197 x 178\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            layer = self.layers[i]\n",
    "            if i in self.layers_of_interest:\n",
    "                input = x\n",
    "                x = layer.ln_1(x)\n",
    "                x, _ = layer.self_attention(x, x, x, need_weights=False)\n",
    "                self_attention = x\n",
    "                x = x + input\n",
    "        \n",
    "                y = layer.ln_2(x)\n",
    "                # 1 x 768\n",
    "                y = layer.mlp[0](y) ## Linear\n",
    "                y = layer.mlp[1](y) ## GELU\n",
    "                # 1 x 3072\n",
    "                gelu = y\n",
    "                y = layer.mlp[3](y) ## Linear\n",
    "                \n",
    "                x = x + y\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "\n",
    "        # Feel free to experiment with different layers.\n",
    "        vit_outputs = namedtuple(\"ViTOutputs\", self.layer_names)\n",
    "        out = vit_outputs(self_attention, gelu)\n",
    "        return out\n",
    "\n",
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, show_progress=False, checkpoint=\"ViT-B/16\"):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        model, process_image = clip.load(checkpoint, device=DEVICE)\n",
    "        self.model = model\n",
    "        self.input_resolution = model.visual.input_resolution\n",
    "        #self.process_image = process_image\n",
    "        self.layer_names = [\"logits_per_image\"]\n",
    "        \n",
    "        \n",
    "        if not requires_grad:\n",
    "          for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        img, text = x\n",
    "        # img = self.process_image(img).to(DEVICE)\n",
    "        img = img.to(DEVICE)\n",
    "        text = clip.tokenize(text).to(DEVICE)\n",
    "\n",
    "        logits_per_image, logits_per_text = self.model(img, text)\n",
    "        out = logits_per_image\n",
    "        \n",
    "        # Feel free to experiment with different layers.\n",
    "        clip_outputs = namedtuple(\"CLIPOutputs\", self.layer_names)\n",
    "        out = clip_outputs(out)\n",
    "        return out\n",
    "\n",
    "    def process_image(self, img_tensor):\n",
    "        # Ensure img_tensor is a torch tensor\n",
    "        if not torch.is_tensor(img_tensor):\n",
    "            raise ValueError(\"Input image must be a torch tensor\")\n",
    "\n",
    "        # Drop the batch dimension for easier processing\n",
    "        img_tensor = img_tensor.squeeze(0)\n",
    "\n",
    "        # Determine the aspect ratio\n",
    "        _, h, w = img_tensor.shape\n",
    "        aspect_ratio = w / h\n",
    "\n",
    "        if aspect_ratio > 1:\n",
    "            # If width > height, then resize height to self.input_resolution\n",
    "            new_height = self.input_resolution\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "        else:\n",
    "            # If width <= height, then resize width to self.input_resolution\n",
    "            new_width = self.input_resolution\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "\n",
    "        # Resize using torch.nn.functional\n",
    "        img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(new_height, new_width), mode='bicubic', align_corners=True)\n",
    "        img_tensor = img_tensor.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Center crop\n",
    "        target_size = self.input_resolution\n",
    "        startx = w // 2 - (target_size // 2)\n",
    "        starty = h // 2 - (target_size // 2)\n",
    "        img_tensor = img_tensor[:, starty:starty+target_size, startx:startx+target_size]\n",
    "\n",
    "        # Reshape to [1, 3, w, h]\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "        return img_tensor\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a6080-41bc-4c8b-b9de-07e8a22fcaa5",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9693a5c-12f9-488e-9c2f-1de5b3393fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdream\n",
    "import importlib\n",
    "\n",
    "importlib.reload(deepdream)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(constants)\n",
    "\n",
    "from utils.constants import *\n",
    "from utils.utils import *\n",
    "import utils.utils as utils\n",
    "\n",
    "from deepdream import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab4ea2a-828a-4274-9198-ce685b52840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = SupportedModels.OPENCLIP_ROBERTA_VIT_B_32.name\n",
    "pretrained_weights = None\n",
    "layers = [\"logits_per_image\"]\n",
    "\n",
    "if \"CLIP\" in model_name:\n",
    "    ConstantsContext.use_clip()\n",
    "\n",
    "# Only a small subset is exposed by design to avoid cluttering\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Common params\n",
    "parser.add_argument(\"--input\", type=str, help=\"Input IMAGE or VIDEO name that will be used for dreaming\", default='figures.jpg')\n",
    "parser.add_argument(\"--img_dimensions\", type=int, help=\"Resize input image to this width\", default=FixedImageResolutions[model_name])\n",
    "parser.add_argument(\"--layers_to_use\", type=str, nargs='+', help=\"Layer whose activations we should maximize while dreaming\", default=layers)\n",
    "parser.add_argument(\"--text_prompt\", type=str, nargs='+', help=\"Text prompt whose CLIP similaruty we should maximize while dreaming\", default=\"A Monkey\")\n",
    "parser.add_argument(\"--model_name\", choices=[m.name for m in SupportedModels],\n",
    "                    help=\"Neural network (model) to use for dreaming\", default=model_name)\n",
    "parser.add_argument(\"--pretrained_weights\", choices=[pw.name for pw in SupportedPretrainedWeights],\n",
    "                    help=\"Pretrained weights to use for the above model\", default=pretrained_weights)\n",
    "\n",
    "# Main params for experimentation (especially pyramid_size and pyramid_ratio)\n",
    "parser.add_argument(\"--pyramid_size\", type=int, help=\"Number of images in an image pyramid\", default=5)\n",
    "parser.add_argument(\"--pyramid_ratio\", type=float, help=\"Ratio of image sizes in the pyramid\", default=1.2)\n",
    "parser.add_argument(\"--num_gradient_ascent_iterations\", type=int, help=\"Number of gradient ascent iterations\", default=10)\n",
    "parser.add_argument(\"--lr\", type=float, help=\"Learning rate i.e. step size in gradient ascent\", default=0.09)\n",
    "\n",
    "# You usually won't need to change these as often\n",
    "parser.add_argument(\"--should_display\", type=bool, help=\"Display intermediate dreaming results\", default=False)\n",
    "parser.add_argument(\"--spatial_shift_size\", type=int, help='Number of pixels to randomly shift image before grad ascent', default=2\n",
    "                   )\n",
    "parser.add_argument(\"--smoothing_coefficient\", type=float, help='Directly controls standard deviation for gradient smoothing', default=0.5)\n",
    "parser.add_argument(\"--use_noise\", type=bool, help=\"Use noise as a starting point instead of input image\", default=False)\n",
    "args = parser.parse_args('')  # important to put '' in Jupyter otherwise it will complain\n",
    "\n",
    "# Wrapping configuration into a dictionary\n",
    "config = dict()\n",
    "for arg in vars(args):\n",
    "    config[arg] = getattr(args, arg)\n",
    "config['dump_dir'] = os.path.join(OUT_IMAGES_PATH, f'{config[\"model_name\"]}_{config[\"pretrained_weights\"]}')\n",
    "config['input'] = os.path.basename(config['input'])  # handle absolute and relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fc13e-5134-4449-b9dd-ebc15233571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepdream import *\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "input_img_name = 'figures.jpg'  # checked in, I'll be using it as the running example\n",
    "img_path = os.path.join(INPUT_DATA_PATH, input_img_name)\n",
    "img = load_image(img_path, target_shape = FixedImageResolutions[model_name])\n",
    "#img = Image.open(img_path)\n",
    "\n",
    "dream_img = deep_dream_static_image(config, None)  # yep a single liner\n",
    "\n",
    "print(img.shape, dream_img.shape)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(2,1, figsize = (10, 15))\n",
    "axs[0].imshow(img)\n",
    "axs[1].imshow(dream_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e4fa7-e850-43e6-8677-14fbe9ee33c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdream",
   "language": "python",
   "name": "deepdream"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
